from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import Qt
from PyQt5.QtWidgets import (QTableWidget, QApplication, QMainWindow, 
                            QTableWidgetItem, QFileSystemModel, QFileDialog,
                            QVBoxLayout, QWidget, QDialog, QMessageBox)
from PyQt5.uic import loadUi

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

from scipy.stats import mode
from sklearn.cluster import DBSCAN, KMeans
from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, 
                             StackingClassifier, VotingClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (confusion_matrix, precision_score, recall_score, f1_score, 
                            roc_curve, roc_auc_score, precision_recall_curve, auc, matthews_corrcoef)
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.semi_supervised import LabelPropagation
from sklearn.svm import OneClassSVM, SVC
from sklearn.tree import DecisionTreeClassifier

import logging
import matplotlib.pyplot as plt
import numpy as np

import pandas as pd
import seaborn as sns

from MSC_Project_GUI import Ui_MainWindow




class ReadOnlyDelegate(QtWidgets.QStyledItemDelegate):
    """
    Delegate class to make table items read-only in QTableWidget.
    """
    
    def createEditor(self, parent, option, index):
        return

# main window class

class Main(QMainWindow, Ui_MainWindow):
    
    """
    Main GUI window for the GUI-Powered Classification Pipeline.
    This class manages data upload, preprocessing, model training, evaluation, and results display.
    """
    
    def __init__(self):
        
        """Initialize GUI components, configure logging, and set up initial UI state."""
        
        super().__init__()
        
        self.setupUi(self)
        
        self.df = None  # To hold the DataFrame
        
        # Set window title and configure logging

        self.setWindowTitle("GUI-Powered Classification Pipeline")
        
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        
        # Formatter and file handler for logging errors
        
        self.formatter = logging.Formatter('%(asctime)s:%(name)s:%(message)s')
        
        self.file_handler = logging.FileHandler('msc_project_error_Main_class.log')
        self.file_handler.setLevel(logging.ERROR)
        self.file_handler.setFormatter(self.formatter)
        
        self.logger.addHandler(self.file_handler)
        
        # Configure table to display data
        self.tableWidget.setRowCount(100)
        self.tableWidget.setColumnCount(31)
        
        # Connect UI buttons to their corresponding functions
        self.upload_button.clicked.connect(self.upload_button_function)
        self.data_scaling_button.clicked.connect(self.scaleData)
        self.treat_outliers_button.clicked.connect(self.treatOutliers)
        self.data_split_button.clicked.connect(self.splitData)
        self.resample_button.clicked.connect(self.resampleData)
        self.build_models_button.clicked.connect(self.run_models)
        
        # Display initial instructions to guide the user
        self.display_instructions()
        
    def display_instructions(self):
        """Display a message box with application usage instructions."""
        try:
            msg = QMessageBox()
            msg.setIcon(QMessageBox.Information)
            msg.setText("Instructions on how to run the application:")
            msg.setInformativeText("1. Load a CSV file.\n"
                                   "2. This applications only builds classification models not regression.\n"
                                   "3. Ensure that the target variable is named 'Class'.\n"
                                   "4. The plots generated by the models are saved in the directory the application is executed.\n"
                                   "5. Ensure the sequence is followed in the order designed by the application.\n"
                                   "6. The models take time to be executed especially for large datasets so it might take some patience to get results.\n"
                                   " THANK YOU!!")
            msg.setWindowTitle("Instructions")
            msg.setStandardButtons(QMessageBox.Ok)
            msg.exec_()
        except Exception as e:
            self.logger.error(f"Error displaying instructions: {str(e)}")

    
        
    def upload_button_function(self):
        """Open file dialog to upload a CSV file and preview its contents."""
        try:
            options = QFileDialog.Options()
            options |= QFileDialog.ReadOnly
            filePath, _ = QFileDialog.getOpenFileName(self, "Open CSV File", "", "CSV Files (*.csv);;All Files (*)", options=options)
            
            if filePath:
                # Display file path in UI and load data into DataFrame
                self.textEdit.setText(filePath)
                self.df = pd.read_csv(filePath)
                # Display a preview of the data
                self.previewData(self.df)
                # Update UI labels with row and column counts
                self.no_row_label.setText(f"{self.df.shape[0]}")
                self.no_column_label.setText(f"{self.df.shape[1]}")
        except Exception as e:
            self.logger.error(f"Error during file upload: {str(e)}")
            
            
    def previewData(self, df):
        try:
            # Displaying only the first 100 rows
            df = df.head(100)
            
            self.tableWidget.setRowCount(df.shape[0])
            self.tableWidget.setColumnCount(df.shape[1])
    
            # Set the table headers
            self.tableWidget.setHorizontalHeaderLabels(df.columns)
    
            # Populate QTableWidget with DataFrame values
            for row in range(df.shape[0]):
                for col in range(df.shape[1]):
                    self.tableWidget.setItem(row, col, QTableWidgetItem(str(df.iat[row, col])))
        except Exception as e:
            self.logger.error(f"Error during data preview: {str(e)}")

                
    def previewData_pre_process(self, df):
        """Display the first 100 rows of the DataFrame in the tableWidget."""
        try:
            # Displaying only the first 100 rows
            df = df.head(100)
            
            self.tableWidget_2.setRowCount(df.shape[0])
            self.tableWidget_2.setColumnCount(df.shape[1])
    
            # Set the table headers
            self.tableWidget_2.setHorizontalHeaderLabels(df.columns)
            
            # Populate QTableWidget with DataFrame values
            for row in range(df.shape[0]):
                for col in range(df.shape[1]):
                    self.tableWidget_2.setItem(row, col, QTableWidgetItem(str(df.iat[row, col])))
                    
            self.pre_process_row_no.setText(f"{self.df.shape[0]}")
            self.pre_process_column_no.setText(f"{self.df.shape[1]}")
        except Exception as e:
            self.logger.error(f"Error during data preview (pre-process): {str(e)}")
           
            
    def scaleData(self):
        """Scale numeric columns using RobustScaler and update displayed DataFrame."""
        
        try:
            if self.df is not None:
                rob_scaler = RobustScaler()
                
                # Scale 'Time' and 'Amount' columns if present
                if 'Amount' in self.df.columns:
                    self.df['scaled_amount'] = rob_scaler.fit_transform(self.df['Amount'].values.reshape(-1,1))
                if 'Time' in self.df.columns:
                    self.df['scaled_time'] = rob_scaler.fit_transform(self.df['Time'].values.reshape(-1,1))
                
                # Drop original 'Time' and 'Amount' columns if they exist
                self.df.drop(['Time', 'Amount'], axis=1, errors='ignore', inplace=True)
        
                # Scale all numeric columns in the dataset
                numeric_cols = self.df.select_dtypes(include=['float64', 'int64']).columns
                for col in numeric_cols:
                    self.df[col] = rob_scaler.fit_transform(self.df[col].values.reshape(-1,1))
        
                self.previewData_pre_process(self.df)
        except Exception as e:
            self.logger.error(f"Error during data scaling: {str(e)}")
            
    
    
    def treatOutliers(self):
        """
        Identifies and removes outliers in the dataset using the IQR method.
        Specifically checks for columns 'V14', 'V12', and 'V10' if present,
        or applies outlier treatment to all numeric columns if they are not.
        Displays information about the process in the output box.
        """
        try:
            # Clear previous output messages
            self.outputBox.clear()
    
            # Check if there is a loaded DataFrame to work with
            if self.df is None:
                self.outputBox.append("No DataFrame loaded. Please load a CSV file first.")
                return
    
            new_df = self.df.copy()  # Make a copy of the DataFrame to retain original data
    
            # Check if specific columns ('V14', 'V12', 'V10') are present in the DataFrame
            if all(column in new_df.columns for column in ['V14', 'V12', 'V10']):
                # Apply targeted outlier treatment for each column
                self._remove_outliers_from_column(new_df, 'V14', label="Feature V14 Outliers for Fraud Cases")
                self._remove_outliers_from_column(new_df, 'V12', label="Feature V12 Outliers for Fraud Cases")
                self._remove_outliers_from_column(new_df, 'V10', label="Feature V10 Outliers for Fraud Cases")
            else:
                # Apply general outlier treatment for all numeric columns
                for col in new_df.select_dtypes(include=[np.number]).columns:
                    self._remove_outliers_from_column(new_df, col, label=f"Feature {col} Outliers")
    
            # Update the main DataFrame and display the pre-processed data
            self.df = new_df
            self.previewData_pre_process(self.df)
    
        except Exception as e:
            # Log the exception if any error occurs
            self.logger.error(f"Error during outlier treatment: {str(e)}")


    def _remove_outliers_from_column(self, df, column, label):
        """
        Helper function to remove outliers from a specified column using IQR.
        
        Parameters:
        - df: DataFrame in which outliers are to be removed.
        - column: Name of the column to process.
        - label: Custom label for displaying outlier information in the output box.
        """
        # Retrieve values of the specified column for rows with Class == 1
        column_values = df[column].loc[df['Class'] == 1].values
    
        # Calculate the 25th (Q1) and 75th (Q3) percentiles
        q25, q75 = np.percentile(column_values, 25), np.percentile(column_values, 75)
        iqr = q75 - q25  # Calculate interquartile range (IQR)
        
        # Define outlier bounds
        cut_off = iqr * 1.5
        lower_bound, upper_bound = q25 - cut_off, q75 + cut_off
    
        # Display IQR, cutoff, and bounds in the output box
        self.outputBox.append(f"{column} Quartile 25: {q25} | Quartile 75: {q75}")
        self.outputBox.append(f"{column} IQR: {iqr}")
        self.outputBox.append(f"{column} Cut Off: {cut_off}")
        self.outputBox.append(f"{column} Lower Bound: {lower_bound}")
        self.outputBox.append(f"{column} Upper Bound: {upper_bound}")
    
        # Identify and display outliers in the output box
        outliers = [x for x in column_values if x < lower_bound or x > upper_bound]
        self.outputBox.append(f"{label}: {len(outliers)} outliers found.")
        self.outputBox.append(f"{column} outliers: {outliers}")
    
        # Remove outliers from the DataFrame
        df.drop(df[(df[column] > upper_bound) | (df[column] < lower_bound)].index, inplace=True)
        self.outputBox.append(f"Remaining instances after {column} outliers removal: {len(df)}")
        self.outputBox.append('----' * 10)  # Separator for readability

      
    
    def splitData(self):
        """
        Splits the loaded DataFrame into training, validation, and test sets based on the user-defined percentage.
        This function reads the test and validation split percentages, splits the data accordingly, 
        and displays the resulting dataset shapes in the output box.
        """
        try:
            # Clear previous output messages
            self.outputBox.clear()
            
            # Retrieve and calculate the test/validation split percentage
            split_percentage = (float(self.splitLineEdit_test.text()) + float(self.splitLineEdit_validation.text())) / 100.0
            
            # Split data into training and temporary datasets (which will be further split into validation and test sets)
            self.df_train, df_temp = train_test_split(self.df, test_size=split_percentage, random_state=42)
            
            # Further split the temporary dataset into validation and test sets with an equal split
            self.df_val, self.df_test = train_test_split(df_temp, test_size=0.5, random_state=42)
            
            # Display the shapes of the training, validation, and test sets in the output box
            self.outputBox.append("Training set shape: {}".format(self.df_train.shape))
            self.outputBox.append("Validation set shape: {}".format(self.df_val.shape))
            self.outputBox.append("Test set shape: {}".format(self.df_test.shape))
    
        except ValueError:
            # Handle case where split percentages are invalid or improperly formatted
            self.outputBox.append("Please provide a valid percentage for the data split.")
        except Exception as e:
            # Log any unexpected errors during the splitting process
            self.logger.error(f"Error during data split: {str(e)}")


    def resampleData(self):
        """
        Resamples the training data to address class imbalance using either oversampling or undersampling.
        The method resamples based on user selection (either oversampling or undersampling) and updates the
        training data accordingly. If neither is selected, no resampling is performed.
        """
        try:
            # Clear previous output messages
            self.outputBox.clear()
            
            # Ensure there is a training dataset to resample
            if self.df_train is None:
                self.outputBox.append("No training data available. Please load and split the data first.")
                return
    
            # Separate features and target labels from the training data
            X = self.df_train.drop('Class', axis=1)
            y = self.df_train['Class']
    
            X_resampled, y_resampled = None, None  # Initialize resampled variables
    
            # Perform oversampling if the corresponding checkbox is selected
            if self.oversampling_checkbox.isChecked():
                ros = RandomOverSampler(random_state=42)
                X_resampled, y_resampled = ros.fit_resample(X, y)
                self.outputBox.append("Oversampled class distribution:\n{}".format(pd.Series(y_resampled).value_counts()))
    
            # Perform undersampling if the corresponding checkbox is selected
            elif self.undersampling_checkbox.isChecked():
                rus = RandomUnderSampler(random_state=42)
                X_resampled, y_resampled = rus.fit_resample(X, y)
                self.outputBox.append("Undersampled class distribution:\n{}".format(pd.Series(y_resampled).value_counts()))
    
            else:
                # If no resampling method is selected, notify the user
                self.outputBox.append("No resampling method selected.")
    
            # Update training data with resampled data if resampling was applied
            if X_resampled is not None and y_resampled is not None:
                self.df_train_resampled = pd.concat([X_resampled, pd.Series(y_resampled, name='Class')], axis=1)
                self.X_train = X_resampled
                self.y_train = y_resampled
            else:
                # If no resampling, set training data as-is
                self.X_train = self.df_train.drop('Class', axis=1)
                self.y_train = self.df_train['Class']
    
            # Separate features and labels for validation and test sets
            self.X_val = self.df_val.drop('Class', axis=1)
            self.y_val = self.df_val['Class']
            self.X_test = self.df_test.drop('Class', axis=1)
            self.y_test = self.df_test['Class']
    
        except Exception as e:
            # Log any unexpected errors during the resampling process
            self.logger.error(f"Error during data resampling: {str(e)}")


    
    def runLogisticRegression(self):
        """
        Trains a Logistic Regression model using GridSearchCV for hyperparameter tuning,
        evaluates the model on the test set, and logs the evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Logistic Regression'
            model = LogisticRegression()  # Initialize the Logistic Regression model
            params = {'C': [1, 10, 100]}  # Hyperparameters to be tuned
    
            # Set up and fit GridSearchCV with cross-validation on validation data
            clf = GridSearchCV(model, params, cv=5, refit=True)
            clf.fit(self.X_val, self.y_val)  # Train on validation data for tuning
    
            # Predictions and probability scores for the test data
            y_pred_test = clf.predict(self.X_test)
            y_prob_test = clf.predict_proba(self.X_test)[:, 1]  # Probability scores for ROC and PR curves
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title='Confusion Matrix for Logistic Regression')
            self.saveROCCurve(y_prob_test, title='ROC Curve for Logistic Regression')
            self.savePrecisionRecallCurve(y_prob_test, title='Precision-Recall Curve for Logistic Regression')
    
        except Exception as e:
            self.logger.error(f"Error during Logistic Regression: {str(e)}")
    
    
    def runDecisionTree(self):
        """
        Trains a Decision Tree classifier using GridSearchCV for hyperparameter tuning,
        evaluates the model on the test set, and logs the evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Decision Tree'
            model = DecisionTreeClassifier()  # Initialize the Decision Tree classifier
            params = {'criterion': ['gini', 'entropy']}  # Hyperparameters to be tuned
    
            # Set up and fit GridSearchCV with cross-validation on validation data
            clf = GridSearchCV(model, params, cv=5, refit=True)
            clf.fit(self.X_val, self.y_val)  # Train on validation data for tuning
    
            # Predictions and probability scores for the test data
            y_pred_test = clf.predict(self.X_test)
            y_prob_test = clf.predict_proba(self.X_test)[:, 1]  # Probability scores for ROC and PR curves
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title='Confusion Matrix for Decision Tree')
            self.saveROCCurve(y_prob_test, title='ROC Curve for Decision Tree')
            self.savePrecisionRecallCurve(y_prob_test, title='Precision-Recall Curve for Decision Tree')
    
        except Exception as e:
            self.logger.error(f"Error during Decision Tree: {str(e)}")
    
    
    def runRandomForest(self):
        """
        Trains a Random Forest classifier using GridSearchCV for hyperparameter tuning,
        evaluates the model on the test set, and logs the evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Random Forest'
            model = RandomForestClassifier()  # Initialize the Random Forest model
            params = {'n_estimators': [10, 50, 100]}  # Hyperparameters to be tuned
    
            # Set up and fit GridSearchCV with cross-validation on validation data
            clf = GridSearchCV(model, params, cv=5, refit=True)
            clf.fit(self.X_val, self.y_val)  # Train on validation data for tuning
    
            # Predictions and probability scores for the test data
            y_pred_test = clf.predict(self.X_test)
            y_prob_test = clf.predict_proba(self.X_test)[:, 1]  # Probability scores for ROC and PR curves
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title='Confusion Matrix for Random Forest')
            self.saveROCCurve(y_prob_test, title='ROC Curve for Random Forest')
            self.savePrecisionRecallCurve(y_prob_test, title='Precision-Recall Curve for Random Forest')
    
        except Exception as e:
            self.logger.error(f"Error during Random Forest execution: {str(e)}")
    
    
    def runGradientBoostedTrees(self):
        """
        Trains a Gradient Boosting classifier using GridSearchCV for hyperparameter tuning,
        evaluates the model on the test set, and logs the evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Gradient Boosted Trees'
            model = GradientBoostingClassifier()  # Initialize the Gradient Boosting model
            params = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.2]}  # Hyperparameters to be tuned
    
            # Set up and fit GridSearchCV with cross-validation on validation data
            clf = GridSearchCV(model, params, cv=5, refit=True)
            clf.fit(self.X_val, self.y_val)  # Train on validation data for tuning
    
            # Predictions and probability scores for the test data
            y_pred_test = clf.predict(self.X_test)
            y_prob_test = clf.predict_proba(self.X_test)[:, 1]  # Probability scores for ROC and PR curves
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title='Confusion Matrix for Gradient Boosted Trees')
            self.saveROCCurve(y_prob_test, title='ROC Curve for Gradient Boosted Trees')
            self.savePrecisionRecallCurve(y_prob_test, title='Precision-Recall Curve for Gradient Boosted Trees')
    
        except Exception as e:
            self.logger.error(f"Error during Gradient Boosted Trees execution: {str(e)}")
    
    
    def runSVM(self):
        """
        Trains a Support Vector Machine (SVM) classifier using GridSearchCV for hyperparameter tuning,
        evaluates the model on the test set, and logs the evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'SVM'
            model = SVC(probability=True)  # Initialize the SVM model with probability support
            params = {'C': [1, 10, 100], 'kernel': ['linear', 'rbf']}  # Hyperparameters to be tuned
    
            # Set up and fit GridSearchCV with cross-validation on validation data
            clf = GridSearchCV(model, params, cv=5, refit=True)
            clf.fit(self.X_val, self.y_val)  # Train on validation data for tuning
    
            # Predictions and probability scores for the test data
            y_pred_test = clf.predict(self.X_test)
            y_prob_test = clf.predict_proba(self.X_test)[:, 1]  # Probability scores for ROC and PR curves
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title='Confusion Matrix for SVM')
            self.saveROCCurve(y_prob_test, title='ROC Curve for SVM')
            self.savePrecisionRecallCurve(y_prob_test, title='Precision-Recall Curve for SVM')
    
        except Exception as e:
            self.logger.error(f"Error during SVM execution: {str(e)}")


    def runKMeans(self):
        """
        Trains a KMeans clustering model using GridSearchCV for hyperparameter tuning,
        evaluates the model by assigning labels based on the majority class, and logs evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'KMeans'
            model = KMeans(random_state=42)  # Initialize the KMeans model
            params = {'n_clusters': [2, 3, 4], 'init': ['k-means++', 'random']}  # Hyperparameters to be tuned
    
            # Set up and fit GridSearchCV on training data for tuning
            clf = GridSearchCV(model, params, cv=5, refit=True)
            clf.fit(self.X_train)  # Train on training data
    
            # Predictions for test data
            y_pred_test = clf.predict(self.X_test)
    
            # Map predicted clusters to the majority class
            labels = np.zeros_like(y_pred_test)
            for i in np.unique(y_pred_test):
                mask = (y_pred_test == i)
                labels[mask] = np.argmax(np.bincount(self.y_test[mask]))
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, labels)
            recall = recall_score(self.y_test, labels)
            f1 = f1_score(self.y_test, labels)
            mcc = matthews_corrcoef(self.y_test, labels)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(labels, title=f'Confusion Matrix for {name}')
            
            # Calculate distances for ROC and PR curves
            distances = clf.transform(self.X_test)
            y_scores = distances.min(axis=1)
            
            self.saveROCCurve(-y_scores, title=f'ROC Curve for {name}')
            self.savePrecisionRecallCurve(-y_scores, title=f'Precision-Recall Curve for {name}')
    
        except Exception as e:
            self.logger.error(f"Error during KMeans execution: {str(e)}")
    
    
    def runOneClassSVM(self):
        """
        Trains a One-Class SVM model for anomaly detection with GridSearchCV hyperparameter tuning,
        evaluates the model on the test set, and logs evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'One-Class SVM'
            
            # Step 1: Fit initial model on training data where class is normal (0)
            one_class_svm = OneClassSVM(kernel='rbf', nu=0.5)
            one_class_svm.fit(self.X_train[self.y_train == 0])
    
            # Step 2: Hyperparameter tuning with GridSearchCV on validation set
            param_grid = {'kernel': ['linear', 'rbf'], 'nu': [0.1, 0.5, 0.9]}
            grid_svm = GridSearchCV(one_class_svm, param_grid, scoring='f1', cv=5)
            grid_svm.fit(self.X_val[self.y_val == 0])
            best_params = grid_svm.best_params_
    
            # Step 3: Evaluate the final model on test data
            best_one_class_svm = OneClassSVM(kernel=best_params['kernel'], nu=best_params['nu'])
            best_one_class_svm.fit(self.X_train[self.y_train == 0])
            
            test_predictions = best_one_class_svm.predict(self.X_test)
            test_predictions[test_predictions == 1] = 0
            test_predictions[test_predictions == -1] = 1
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, test_predictions)
            recall = recall_score(self.y_test, test_predictions)
            f1 = f1_score(self.y_test, test_predictions)
            mcc = matthews_corrcoef(self.y_test, test_predictions)
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': best_params,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(test_predictions, title=f'Confusion Matrix for {name}')
            
            # Decision function for ROC and PR curves
            y_score_test = best_one_class_svm.decision_function(self.X_test)
            
            self.saveROCCurve(y_score_test, title=f'ROC Curve for {name}')
            self.savePrecisionRecallCurve(y_score_test, title=f'Precision-Recall Curve for {name}')
    
        except Exception as e:
            self.logger.error(f"Error during One-Class SVM execution: {str(e)}")
    
    
    def runDBSCAN(self):
        """
        Trains a DBSCAN clustering model, evaluates by assigning labels based on the majority class in clusters,
        and logs evaluation metrics. Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'DBSCAN'
            
            # Step 1: Standardize the training and test data
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(self.X_train)
            X_test_scaled = scaler.transform(self.X_test)
    
            # Step 2: Train DBSCAN model on combined train and test sets for label assignment
            model = DBSCAN(eps=0.5, min_samples=5)
            model.fit(np.vstack((X_train_scaled, X_test_scaled)))
            
            # Extract predictions for test data
            labels_total = model.labels_
            y_pred_test = labels_total[len(X_train_scaled):]
    
            # Map clusters to the majority class for consistency
            majority_label = mode(self.y_test)[0][0]
            for cluster_label in np.unique(y_pred_test):
                if cluster_label == -1:
                    continue  # Skip noise points
                mask = (y_pred_test == cluster_label)
                y_pred_test[mask] = majority_label if mode(self.y_test[mask])[0][0] == majority_label else 1 - majority_label
    
            # Step 3: Calculate distances to nearest core sample for ROC/PR curves
            nn = NearestNeighbors(n_neighbors=2)
            nn.fit(X_train_scaled)
            distances, _ = nn.kneighbors(X_test_scaled)
            y_score_test = -distances[:, 0]
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test, average='weighted')
            recall = recall_score(self.y_test, y_pred_test, average='weighted')
            f1 = f1_score(self.y_test, y_pred_test, average='weighted')
    
            # Log the model results
            self.results.append({
                'Model': name,
                'Best Params': 'NA',
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title=f'Confusion Matrix for {name}')
            self.saveROCCurve(y_score_test, title=f'ROC Curve for {name}')
            self.savePrecisionRecallCurve(y_score_test, title=f'Precision-Recall Curve for {name}')
    
        except Exception as e:
            self.logger.error(f"Error during DBSCAN execution: {str(e)}")

    
    def runVotingClassifier(self):
        """
        Trains a Voting Classifier ensemble with Logistic Regression, Random Forest, and Gradient Boosting,
        applies GridSearchCV for hyperparameter tuning, evaluates on test set, and logs evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Voting Classifier'
            
            # Define individual classifiers
            lr = LogisticRegression()
            rf = RandomForestClassifier()
            gbc = GradientBoostingClassifier()
    
            # Initialize voting classifier with soft voting
            voting_clf = VotingClassifier(estimators=[
                ('lr', lr), ('rf', rf), ('gbc', gbc)], voting='soft')
    
            # Fit initial voting classifier model
            voting_clf.fit(self.X_train, self.y_train)
    
            # Set up GridSearchCV for hyperparameter tuning on the validation set
            param_grid = {
                'lr__C': [1, 10],
                'rf__n_estimators': [50, 100],
                'gbc__n_estimators': [50, 100],
                'gbc__learning_rate': [0.01, 0.1],
            }
            grid_clf = GridSearchCV(voting_clf, param_grid, scoring='f1', cv=5, n_jobs=-1)
            grid_clf.fit(self.X_val, self.y_val)
    
            # Make predictions on the test set
            y_pred_test = grid_clf.predict(self.X_test)
            y_prob_test = grid_clf.predict_proba(self.X_test)[:, 1]
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log model results
            self.results.append({
                'Model': name,
                'Best Params': grid_clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title=f'Confusion Matrix for {name}')
            self.saveROCCurve(y_prob_test, title=f'ROC Curve for {name}')
            self.savePrecisionRecallCurve(y_prob_test, title=f'Precision-Recall Curve for {name}')
    
        except Exception as e:
            self.logger.error(f"Error during Voting Classifier execution: {str(e)}")
    
    
    def runStackingClassifier(self):
        """
        Trains a Stacking Classifier ensemble with Logistic Regression, Random Forest, and Gradient Boosting,
        applies GridSearchCV for hyperparameter tuning, evaluates on test set, and logs evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Stacking Classifier'
    
            # Define base classifiers
            lr = LogisticRegression()
            rf = RandomForestClassifier()
            gbc = GradientBoostingClassifier()
    
            # Initialize stacking classifier with logistic regression as the final estimator
            stacking_clf = StackingClassifier(estimators=[
                ('lr', lr), ('rf', rf), ('gbc', gbc)], final_estimator=LogisticRegression())
    
            # Fit initial stacking classifier model
            stacking_clf.fit(self.X_train, self.y_train)
    
            # Set up GridSearchCV for hyperparameter tuning on the validation set
            param_grid = {
                'lr__C': [1, 10],
                'rf__n_estimators': [50, 100],
                'gbc__n_estimators': [50, 100],
                'gbc__learning_rate': [0.01, 0.1],
                'final_estimator__C': [0.1, 1.0],
            }
            grid_clf = GridSearchCV(stacking_clf, param_grid, scoring='f1', cv=5, n_jobs=-1)
            grid_clf.fit(self.X_val, self.y_val)
    
            # Make predictions on the test set
            y_pred_test = grid_clf.predict(self.X_test)
            y_prob_test = grid_clf.predict_proba(self.X_test)[:, 1]
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log model results
            self.results.append({
                'Model': name,
                'Best Params': grid_clf.best_params_,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title=f'Confusion Matrix for {name}')
            self.saveROCCurve(y_prob_test, title=f'ROC Curve for {name}')
            self.savePrecisionRecallCurve(y_prob_test, title=f'Precision-Recall Curve for {name}')
    
        except Exception as e:
            self.logger.error(f"Error during Stacking Classifier execution: {str(e)}")
    
    
    def runLabelPropagation(self):
        """
        Trains a semi-supervised Label Propagation model using k-Nearest Neighbors kernel on partially labeled training data,
        evaluates the model on the test set, and logs evaluation metrics.
        Generates and saves visualizations (Confusion Matrix, ROC Curve, and Precision-Recall Curve).
        """
        try:
            name = 'Label Propagation'
    
            # Randomly mask part of the labels for semi-supervised learning
            frac_labeled = 0.5  # Proportion of labels to keep
            mask_labeled = np.random.choice([True, False], len(self.y_train), p=[frac_labeled, 1 - frac_labeled])
            y_train_partial = self.y_train.copy()
            y_train_partial[~mask_labeled] = -1  # Masked labels set to -1
    
            # Initialize Label Propagation model
            label_prop_model = LabelPropagation(kernel='knn', n_neighbors=7)
            label_prop_model.fit(self.X_train, y_train_partial)  # Train with partial labels
    
            # Make predictions on the test set
            y_pred_test = label_prop_model.predict(self.X_test)
            y_prob_test = label_prop_model.predict_proba(self.X_test)[:, 1]
    
            # Replace NaNs in predicted probabilities (NaNs may occur in semi-supervised learning)
            y_prob_test = np.nan_to_num(y_prob_test, nan=0.5)
    
            # Calculate evaluation metrics
            precision = precision_score(self.y_test, y_pred_test)
            recall = recall_score(self.y_test, y_pred_test)
            f1 = f1_score(self.y_test, y_pred_test)
            mcc = matthews_corrcoef(self.y_test, y_pred_test)
    
            # Log model results
            self.results.append({
                'Model': name,
                'Precision': precision,
                'Recall': recall,
                'F1 Score': f1,
                'MCC': mcc
            })
    
            # Save evaluation visualizations
            self.saveConfusionMatrix(y_pred_test, title=f'Confusion Matrix for {name}')
            self.saveROCCurve(y_prob_test, title=f'ROC Curve for {name}')
            self.savePrecisionRecallCurve(y_prob_test, title=f'Precision-Recall Curve for {name}')
    
        except Exception as e:
            self.logger.error(f"Error during Label Propagation execution: {str(e)}")

    
    def saveConfusionMatrix(self, y_pred_test, title):
        """
        Generates and saves a Confusion Matrix plot based on the true test labels (self.y_test) 
        and the predicted test labels (y_pred_test).
        
        Args:
            y_pred_test (array-like): Predicted labels for the test data.
            title (str): Title for the plot, which is also used as the filename for saving the plot.
        """
        try:
            # Create figure and plot confusion matrix
            plt.figure(figsize=(10, 7))
            conf_mat = confusion_matrix(self.y_test, y_pred_test)
            sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
            
            # Set plot labels and title
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.title(title)
            
            # Save plot to file
            plt.savefig(title + '.png')
            plt.close()  # Close plot to free memory
        except Exception as e:
            self.logger.error(f"Error saving Confusion Matrix plot: {str(e)}")
    
    
    def saveROCCurve(self, y_prob_test, title):
        """
        Generates and saves an ROC Curve plot using the true test labels (self.y_test) and 
        the predicted probabilities for the positive class (y_prob_test).
        
        Args:
            y_prob_test (array-like): Probability estimates of the positive class for the test data.
            title (str): Title for the plot, which is also used as the filename for saving the plot.
        """
        try:
            # Create figure and plot ROC curve
            plt.figure(figsize=(10, 7))
            fpr, tpr, _ = roc_curve(self.y_test, y_prob_test)
            roc_auc = roc_auc_score(self.y_test, y_prob_test)
            
            plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
            plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random model
            
            # Set plot labels, title, and legend
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(title)
            plt.legend(loc="lower right")
            
            # Save plot to file
            plt.savefig(title + '.png')
            plt.close()  # Close plot to free memory
        except Exception as e:
            self.logger.error(f"Error saving ROC Curve plot: {str(e)}")
    
    
    def savePrecisionRecallCurve(self, y_prob_test, title):
        """
        Generates and saves a Precision-Recall Curve plot using the true test labels (self.y_test) 
        and the predicted probabilities for the positive class (y_prob_test).
        
        Args:
            y_prob_test (array-like): Probability estimates of the positive class for the test data.
            title (str): Title for the plot, which is also used as the filename for saving the plot.
        """
        try:
            # Create figure and plot Precision-Recall curve
            plt.figure(figsize=(10, 7))
            precision, recall, _ = precision_recall_curve(self.y_test, y_prob_test)
            pr_auc = auc(recall, precision)
            
            plt.plot(recall, precision, label=f'Precision-Recall curve (area = {pr_auc:.2f})')
            
            # Set plot labels, title, and legend
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(title)
            plt.legend(loc="upper right")
            
            # Save plot to file
            plt.savefig(title + '.png')
            plt.close()  # Close plot to free memory
        except Exception as e:
            self.logger.error(f"Error saving Precision-Recall Curve plot: {str(e)}")
            
            
    
    
    def run_models(self):
        """
        Executes the selected machine learning models based on the status of checkboxes in the PyQt GUI.
        Aggregates the evaluation results for each model and triggers the display of results.
        """
        try:
            # Initialize an empty list to store results
            self.results = []
    
            # Run Logistic Regression if selected
            if self.model_checkbox_1.isChecked():
                self.runLogisticRegression()
    
            # Run Decision Tree if selected
            if self.model_checkbox_2.isChecked():
                self.runDecisionTree()
    
            # Run Random Forest if selected
            if self.model_checkbox_3.isChecked():
                self.runRandomForest()
    
            # Run Gradient Boosting if selected
            if self.model_checkbox_4.isChecked():
                self.runGradientBoostedTrees()
    
            # Run SVM if selected
            if self.model_checkbox_5.isChecked():
                self.runSVM()
    
            # Run KMeans if selected
            if self.model_checkbox_6.isChecked():
                self.runKMeans()
    
            # Run One-Class SVM if selected
            if self.model_checkbox_7.isChecked():
                self.runOneClassSVM()
    
            # Run DBSCAN if selected
            if self.model_checkbox_8.isChecked():
                self.runDBSCAN()
    
            # Run Voting Classifier if selected
            if self.model_checkbox_9.isChecked():
                self.runVotingClassifier()
    
            # Run Stacking Classifier if selected
            if self.model_checkbox_10.isChecked():
                self.runStackingClassifier()
    
            # Run Label Propagation if selected
            if self.model_checkbox_11.isChecked():
                self.runLabelPropagation()
    
            # Display all model results
            self.displayResults()
    
        except Exception as e:
            self.logger.error(f"Error running models: {str(e)}")
    
    
    def displayResults(self):
        """
        Displays the results of the executed models in the results table and highlights the top-performing model 
        in a text box based on the evaluation metrics. 
        """
        try:
            # Convert results to a DataFrame for easier display
            self.results_df = pd.DataFrame(self.results)
    
            # If MCC column exists, sort the DataFrame by MCC in descending order
            if "MCC" in self.results_df.columns:
                self.results_df = self.results_df.sort_values(by="MCC", ascending=False)
    
            # Format numeric columns to two decimal places for readability
            for col in self.results_df.columns:
                if pd.api.types.is_numeric_dtype(self.results_df[col]):
                    self.results_df[col] = self.results_df[col].apply(lambda x: '{:.2f}'.format(x))
    
            # Configure table rows and columns to match the results DataFrame
            self.resultsTable.setRowCount(len(self.results_df))
            self.resultsTable.setColumnCount(len(self.results_df.columns))
    
            # Set table column headers to match DataFrame columns
            self.resultsTable.setHorizontalHeaderLabels(self.results_df.columns)
    
            # Populate the table with data from the DataFrame
            for i in range(len(self.results_df)):
                for j, col_name in enumerate(self.results_df.columns):
                    item = self.results_df.iloc[i, j]
                    self.resultsTable.setItem(i, j, QTableWidgetItem(str(item)))
    
            # Display details of the top-performing model in a text box
            best_model = self.results_df.iloc[0]
            best_model_text = (f"The best model is '{best_model['Model']}' with a precision score of {best_model['Precision']}, "
                               f"recall score of {best_model['Recall']}, f1 score of {best_model['F1 Score']}, "
                               f"and MCC score of {best_model['MCC']}.")
            
            self.resultsTextBox.setText(best_model_text)
    
        except Exception as e:
            self.logger.error(f"Error displaying results: {str(e)}")

        

if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    win = Main()
    win.show()
    sys.exit(app.exec_())

